{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autoencoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP67mvU28oesBZQsE5WE5Cu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajArPatra/Dimentionality-Reduction/blob/main/Autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g2TlknZAIDK"
      },
      "source": [
        "Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34YGe2ritJ0Q",
        "outputId": "97862b9b-5ac1-45e6-ca9e-19ed35b458fe"
      },
      "source": [
        "!gdown https://drive.google.com/u/0/uc?id=1Nl-RU5HggCSWWqINN4gbxHAVzER6T2Po&export=download\n",
        "!gzip -d /content/HIGGS_6M.csv.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=1Nl-RU5HggCSWWqINN4gbxHAVzER6T2Po\n",
            "To: /content/HIGGS_6M.csv.gz\n",
            "1.56GB [00:08, 180MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syWYWjiYAl-r"
      },
      "source": [
        "Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xr6gd8XwWVwN"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "import os\n",
        "import numpy as np\n",
        "from skimage import io,transform as ts\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import imgaug as ia\n",
        "import imgaug.augmenters as iaa\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "#from bhtsne import tsne\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfrOUNLvWYoK"
      },
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VncR-qFwEf1W"
      },
      "source": [
        "Train and Test Split Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcaLuyClKjFe"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "df= pd.read_csv('/content/HIGGS_6M.csv')\n",
        "\n",
        "df=df.replace([np.inf, -np.inf],np.nan)\n",
        "#print(df.isnull().sum())\n",
        "df1=df.iloc[:-500000]\n",
        "df2=df.iloc[-500000:]\n",
        "\n",
        "np.save('train.npy', df1.to_numpy(),True)\n",
        "np.save('test.npy', df2.to_numpy(),True)\n",
        "del df,df1,df2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sINGpnNkBeLT"
      },
      "source": [
        "Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r9pGS0PL-jS"
      },
      "source": [
        "class data1(Dataset):\n",
        "    def __init__(self, train=True,device='cpu'):\n",
        "      self.train=train\n",
        "      self.scaler=MinMaxScaler()\n",
        "      \n",
        "      #self.scaler.fit(np.load('train_num.npy',allow_pickle=True)[:,-1].reshape(-1, 1))\n",
        "      self.device=device\n",
        "      #x_test = np.load(f,allow_pickle=True)\n",
        "      #y_test= np.load('/content/y_test_pred.npy',allow_pickle=True) \n",
        "      #z=int(0.9*X_train.shape[0])\n",
        "      \n",
        "     \n",
        "      if train:\n",
        "        self.data= np.load('train_auto.npy',allow_pickle=True)\n",
        "        self.data_x=self.data[:,1:]\n",
        "        \n",
        "        self.data_y=self.data[:,0]\n",
        "      \n",
        "      else:\n",
        "        self.data= np.load('test_auto.npy',allow_pickle=True)\n",
        "        self.data_x=self.data[:,1:]\n",
        "        \n",
        "        self.data_y=self.data[:,0]\n",
        "      \n",
        "     \n",
        "      \n",
        "    def __getitem__(self, idx):\n",
        "     \n",
        "      #X=scaler.fit_transform(X.reshape(1, -1))\n",
        "      X=torch.from_numpy(self.data_x[idx]).float()\n",
        "      \n",
        "      \n",
        "      \n",
        "      '''if self.train:\n",
        "        y=torch.tensor(self.map[int(self.data_y[idx])])\n",
        "      else:'''\n",
        "      \n",
        "      y=torch.tensor(self.data_y[idx]).long()\n",
        "      \n",
        "     \n",
        "      return X.to(self.device), y.to(self.device)\n",
        "      \n",
        "    def __len__(self):\n",
        "      \n",
        "      return self.data.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4yIk4-a_A9sH",
        "outputId": "7f16fa70-ebcc-41ee-edf5-5fafcf70fd22"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  device='cuda'\n",
        "else :\n",
        "   device='cpu'\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzklR-hv7g06"
      },
      "source": [
        "train_dataset=data1(train=True,device=device)\n",
        "test_dataset=data1(train=False,device=device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfGGI7Du7qOU"
      },
      "source": [
        "train_loader=torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=256,\n",
        "                                           shuffle=True)\n",
        "test_loader=torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                           batch_size=256, \n",
        "                                           shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtUR1NrqBiKn"
      },
      "source": [
        "Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5K5w3s3fRv6"
      },
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self,D_in,H=50,H2=12,latent_dim=3):\n",
        "        \n",
        "        #Encoder\n",
        "        super(Autoencoder,self).__init__()\n",
        "        self.linear1=nn.Linear(D_in,H)\n",
        "        self.lin_bn1 = nn.BatchNorm1d(num_features=H)\n",
        "        self.linear2=nn.Linear(H,H2)\n",
        "        self.lin_bn2 = nn.BatchNorm1d(num_features=H2)\n",
        "        self.linear3=nn.Linear(H2,H2)\n",
        "        self.lin_bn3 = nn.BatchNorm1d(num_features=H2)\n",
        "        \n",
        "#         # Latent vectors mu and sigma\n",
        "        self.fc1 = nn.Linear(H2, latent_dim)\n",
        "        self.bn1 = nn.BatchNorm1d(num_features=latent_dim)\n",
        "        self.fc21 = nn.Linear(latent_dim, latent_dim)\n",
        "        self.fc22 = nn.Linear(latent_dim, latent_dim)\n",
        "\n",
        "#         # Sampling vector\n",
        "        self.fc3 = nn.Linear(latent_dim, latent_dim)\n",
        "        self.fc_bn3 = nn.BatchNorm1d(latent_dim)\n",
        "        self.fc4 = nn.Linear(latent_dim, H2)\n",
        "        self.fc_bn4 = nn.BatchNorm1d(H2)\n",
        "        \n",
        "#         # Decoder\n",
        "        self.linear4=nn.Linear(H2,H2)\n",
        "        self.lin_bn4 = nn.BatchNorm1d(num_features=H2)\n",
        "        self.linear5=nn.Linear(H2,H)\n",
        "        self.lin_bn5 = nn.BatchNorm1d(num_features=H)\n",
        "        self.linear6=nn.Linear(H,D_in)\n",
        "        self.lin_bn6 = nn.BatchNorm1d(num_features=D_in)\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def encode(self, x):\n",
        "        lin1 = self.relu(self.lin_bn1(self.linear1(x)))\n",
        "        lin2 = self.relu(self.lin_bn2(self.linear2(lin1)))\n",
        "        lin3 = self.relu(self.lin_bn3(self.linear3(lin2)))\n",
        "\n",
        "        fc1 = F.relu(self.bn1(self.fc1(lin3)))\n",
        "\n",
        "        r1 = self.fc21(fc1)\n",
        "        r2 = self.fc22(fc1)\n",
        "        \n",
        "        return r1, r2\n",
        "    \n",
        "    def reparameterize(self, mu, logvar):\n",
        "        if self.training:\n",
        "            std = logvar.mul(0.5).exp_()\n",
        "            eps = Variable(std.data.new(std.size()).normal_())\n",
        "            return eps.mul(std).add_(mu)\n",
        "        else:\n",
        "            return mu\n",
        "        \n",
        "    def decode(self, z):\n",
        "        fc3 = self.relu(self.fc_bn3(self.fc3(z)))\n",
        "        fc4 = self.relu(self.fc_bn4(self.fc4(fc3)))\n",
        "\n",
        "        lin4 = self.relu(self.lin_bn4(self.linear4(fc4)))\n",
        "        lin5 = self.relu(self.lin_bn5(self.linear5(lin4)))\n",
        "        return self.lin_bn6(self.linear6(lin5))\n",
        "\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        # self.decode(z) ist spÃ¤ter recon_batch, mu ist mu und logvar ist logvar\n",
        "        return self.decode(z), mu, logvar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO0d945ykZAp"
      },
      "source": [
        "class customLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(customLoss, self).__init__()\n",
        "        self.mse_loss = nn.MSELoss(reduction=\"sum\")\n",
        "    \n",
        "    # x_recon ist der im forward im Model erstellte recon_batch, x ist der originale x Batch, mu ist mu und logvar ist logvar \n",
        "    def forward(self, x_recon, x, mu, logvar):\n",
        "        loss_MSE = self.mse_loss(x_recon, x)\n",
        "        loss_KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "        return loss_MSE + loss_KLD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qqe--OtBsqn"
      },
      "source": [
        "Helper function to Initialise weight"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5edXoGOadL00"
      },
      "source": [
        "from torch.nn import init\n",
        "def init_weights(net, init_type='normal', gain=0.02):\n",
        "    def init_func(m):\n",
        "        classname = m.__class__.__name__\n",
        "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
        "            if init_type == 'normal':\n",
        "                init.normal_(m.weight.data, 0.0, gain)\n",
        "            elif init_type == 'xavier':\n",
        "                init.xavier_normal_(m.weight.data, gain=gain)\n",
        "            elif init_type == 'kaiming':\n",
        "                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "            elif init_type == 'orthogonal':\n",
        "                init.orthogonal_(m.weight.data, gain=gain)\n",
        "            else:\n",
        "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
        "            if hasattr(m, 'bias') and m.bias is not None:\n",
        "                init.constant_(m.bias.data, 0.0)\n",
        "        elif classname.find('BatchNorm2d') != -1:\n",
        "            init.normal_(m.weight.data, 1.0, gain)\n",
        "            init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "    print('initialize network with %s' % init_type)\n",
        "    net.apply(init_func)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtogNjuaCr8T"
      },
      "source": [
        "Save CheckPoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z0qrQ1tX3cF"
      },
      "source": [
        "def save_checkpoint(model, save_folder, name, epoch, opt, best_model=False):\n",
        "\n",
        "    if best_model:\n",
        "        save_path = \"{}/{}/best_model.th\".format(\n",
        "            save_folder, name)\n",
        "    else:\n",
        "        save_path = \"{}/{}/epoch_{}.th\".format(save_folder,\n",
        "                                               name, epoch)\n",
        "\n",
        "    if not os.path.exists(save_folder + \"/\" + name):\n",
        "        os.makedirs(save_folder + \"/\" + name)\n",
        "\n",
        "    print(\"SAVE MODEL to\", save_path)\n",
        "\n",
        "\n",
        "    param_dict = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': opt.state_dict()}\n",
        "        \n",
        "\n",
        "    torch.save(param_dict, save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoJ1RvQjC9h2"
      },
      "source": [
        "Load CheckPoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YNVtZ__R4Fb"
      },
      "source": [
        "def load_checkpoint(model, save_folder, model_name, opt,epoch,device ,best_model=False):\n",
        "      if best_model:\n",
        "        load_path = \"{}/model_{}/best_model.th\".format(save_folder, model_name)\n",
        "      else:\n",
        "        load_path = \"{}/{}/epoch_{}.th\".format(save_folder,\n",
        "                                                model_name, epoch)\n",
        "\n",
        "      checkpoint = torch.load(load_path)\n",
        "\n",
        "      epoch = checkpoint['epoch']\n",
        "      \n",
        "\n",
        "      model =model # code for model init\n",
        "      model.load_state_dict(checkpoint['model_state_dict'])\n",
        "      model=model.to(device)\n",
        "\n",
        "      opt =opt #code for opt init\n",
        "      opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "      #opt.param_groups[0]['lr'] = checkpoint['optimizer_params']['lr']\n",
        "      return model, opt, epoch   \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ug6FY5LQnnk8",
        "outputId": "14f53bf8-be0c-40fa-c8e2-92dcfafe8b39"
      },
      "source": [
        "D_in = 28\n",
        "H = 50\n",
        "H2 = 12\n",
        "model = Autoencoder(D_in, H, H2).to(device)\n",
        "init_weights(model,'kaiming')\n",
        "#model.to(device)\n",
        "#model.apply(weights_init_uniform_rule)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "scheduler=torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)\n",
        "\n",
        "criterion=customLoss()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initialize network with kaiming\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ly7L9sgDVbx"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76-gCnq7orej"
      },
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data,_) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "        loss = criterion(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "#        if batch_idx % log_interval == 0:\n",
        "#            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "#                epoch, batch_idx * len(data), len(trainloader.dataset),\n",
        "#                       100. * batch_idx / len(trainloader),\n",
        "#                       loss.item() / len(data)))\n",
        "    if epoch % 1 == 0:        \n",
        "        print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "            epoch, train_loss / len(train_loader.dataset)))\n",
        "        train_losses.append(train_loss / len(train_loader.dataset))\n",
        "\n",
        "train_losses=[]\n",
        "for i in range(20):\n",
        "  \n",
        "  train(i+lst_epoch+1)\n",
        "  #scheduler.step()\n",
        "  if ((i+lst_epoch+1)%5) ==0:\n",
        "    save_checkpoint(model,'/content/drive/MyDrive/Model_files','dim_reduction',(i+lst_epoch+1),optimizer)\n",
        "  elif i==19:\n",
        "    save_checkpoint(model,'/content/drive/MyDrive/Model_files','dim_reduction',(i+lst_epoch+1),optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNHm7RepF__R"
      },
      "source": [
        "Save Dimention reduced Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kng2FZI6oqZu"
      },
      "source": [
        "mu_output = []\n",
        "logvar_output = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (data,_) in enumerate(train_loader):\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            recon_batch, mu, logvar = model(data)\n",
        "\n",
        "            \n",
        "            mu_tensor = mu   \n",
        "            mu_output.append(mu_tensor)\n",
        "            mu_result = torch.cat(mu_output, dim=0)\n",
        "            \n",
        "            logvar_tensor = logvar   \n",
        "            logvar_output.append(logvar_tensor)\n",
        "            logvar_result = torch.cat(logvar_output, dim=0)\n",
        "\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}